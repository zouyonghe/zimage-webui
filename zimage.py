import torch
from diffusers import ZImagePipeline

# ============================
# æ˜¾å­˜ä¼˜åŒ–è®¾ç½®ï¼ˆå¼ºçƒˆæ¨èï¼‰
# ============================
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# å¦‚æœæ˜¾å­˜ç¢ç‰‡ä¸¥é‡ï¼Œå¼€å¯å¯æ‰©å±•æ˜¾å­˜æ¨¡å¼
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

print("Loading Z-Image-Turbo from local weights...")

pipe = ZImagePipeline.from_pretrained(
    "./zimage-model",
    torch_dtype=torch.bfloat16,    # 4090 æ”¯æŒ BF16ï¼Œéå¸¸ç¨³å®š
    local_files_only=True,
)

# ============================
# å¯ç”¨æ˜¾å­˜ä¼˜åŒ–
# ============================
pipe = pipe.to("cuda")

# xformersï¼šæ˜¾å­˜ -20ï½40%
try:
    pipe.enable_xformers_memory_efficient_attention()
    print("Enabled xformers memory efficient attention.")
except Exception as e:
    print("xformers not available:", e)

# attention slicingï¼šè¿›ä¸€æ­¥é™ä½å³°å€¼æ˜¾å­˜
pipe.enable_attention_slicing()

# ============================
# ç”Ÿæˆå›¾åƒ
# ============================
print("Generating...")
image = pipe(
    "a cat sitting on a chair, high quality, detailed",
    num_inference_steps=9,
    guidance_scale=0.0,

    # ğŸš€ **å…³é”®ï¼šé™ä½åˆ†è¾¨ç‡ï¼Œé˜²æ­¢ 24GB çˆ†æ˜¾å­˜**
    height=512,
    width=512,
).images[0]

image.save("zimage_test.png")
print("Saved: zimage_test.png")
